%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a template file for Web of Conferences Journal
%
% Copy it to a new file with a new name and use it as the basis
% for your article
%
%%%%%%%%%%%%%%%%%%%%%%%%%% EDP Science %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%\documentclass[option]{webofc}
%%% "twocolumn" for typesetting an article in two columns format (default one column)
%
\documentclass{webofc}
\usepackage[varg]{txfonts}   % Web of Conferences font

\usepackage{hyperref}

\usepackage{pgfgantt}
\usepackage{capt-of}
\usepackage{listings}

\lstset{
  frame=single,                    % frame around the code
  %numbers=left,                   % where to put the line-numbers
  stepnumber=1,                   % the step between two line-numbers.
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=false,                % sets automatic line breaking
  breakatwhitespace=true,         % sets if automatic breaks should only happen at whitespace
}


%
% Put here some packages required or/and some personnal commands
%
%
\begin{document}

%
\title{dCache integration with CERN Tape Archive}
%
% subtitle is optionnal
%
%%%\subtitle{Do you have a subtitle?\\ If so, write it here}

\author{
         \firstname{Tigran} \lastname{Mkrtchyan}\inst{1}\fnsep\thanks{\email{tigran.mkrtchyan@desy.de}} \and
        \firstname{Jacek} \lastname{Chodak}\inst{1}\fnsep \and
        \firstname{Mwai} \lastname{Karimi}\inst{1}\fnsep \and
        \firstname{Ralf} \lastname{Lueken}\inst{1}\fnsep \and
        \firstname{Svenja} \lastname{Meyer}\inst{1}\fnsep \and
        \firstname{Peter} \lastname{Suchowski}\inst{1}\fnsep \and
        \firstname{Christian} \lastname{Voss}\inst{1}\fnsep
}

\institute{
Deutsches Elektronen-Synchrotron DESY, Notkestrasse 85, 22607 Hamburg, Germany
}

\abstract{%
  The ever-increasing amount of data that is produced by modern scientific facilities like EuXFEL or LHC puts
  high pressure on the data management infrastructure at the laboratories. This includes poorly shareable resources
  of archival storage, typically, tape libraries. To achieve maximal efficiency of the available tape resources a
  deep integration between hardware and software components is required. The CERN Tape Archive (CTA) is an open-source storage management system developed by CERN to manage LHC experiment
  data on tape. Although today CTA's primary target is CERN Tier-0, the data management group at DESY considers the
  CTA as a main alternative to commercial HSM systems. dCache has an extensible tape interface that allows connectivity to almost any tape system. Together with the CERN Tape Archive developers, the storage team at DESY works on seamless integration of CTA into dCache.
}
%
\maketitle
%
\section{Introduction}
\label{intro}

The ever-increasing amount of data that is produced by modern scientific facilities like EuXFEL or LHC puts high pressure
on the data management infrastructure at the laboratories. The challenges that have to be addressed span over the full data 
life-cycle: from ingest, efficient data analysis, up to long-term preservation. The latter typically utilizes magnetic
tape media, which in combination with disk storage covers the data management requirements. DESY-IT groups use dCache\cite{dcache}, a
storage system developed at DESY in collaboration with Fermilab and Nordic eInfrastructure Collaboration (NeIC), to manage
large numbers of disk servers and transparent data migration to and from archival storage.

Even though today large hard disk-based storage systems are cost, space and volume-effective (1 PB disk storage requires less than a ${1m^{3}}$ of space),
magnetic tapes are still the best option for long-term data archival, especially for so-called cold data, data that are rarely accessed.
Indeed, tape cartridges don't require electric power when not used, can store up to 20TB of uncompressed data and are designed for
15 - 30 years of archival storage. To organize a large number of tape cartridges, tape libraries with robotic arms are used.

There are, of course, some downsides of tape-based technologies. When streaming a modern tape drive provides up to 400 MB/s
and the so-called positioning time on average is about 50 seconds. This doesn't include the mount time, the time which is needed by the
robotic arm to put the tape into an appropriate tape drive. Moreover, magnetic tapes don't support concurrency, which means that only
one stream, e.g. only one file, can be read or written at any point in time.

Thus, to achieve maximal efficiency of the available tape resources, e.g. maximize the time during which tape is read or written and
maximal speed, a deep integration between hardware and software components is required.

Since the early 90's DESY-IT has been relying on Open Storage Manager (OSM) software to operate tape libraries and access data on tapes\cite{osm_desy}.
During these three decades, the OSM underwent many changes. Over 80\% of the original code base has been updated to address computer, network,
operating system and software paradigms that have changed over the decades. Nevertheless, the scaling capabilities put into the original design
don't apply today. Moreover, the commercial software license of OSM doesn't allow us to share our changes with the broader scientific community.
Thus, a new scalable and preferably open-source successor is required.

With dCache as the primary disk layer in front of an HSM, the tape system should fulfil the following requirements:

\begin{itemize}
    \item maximize tape HW efficiency,
    \item integration into DESY ecosystem,
    \item integration with dCache tape interface,
    \item support Enterprise \& LTO,
    \item daily turnover about 1PB,
    \item stable operation for the next decade,
    \item Open-source, adopting open standards,
    \item wide user and technology community,
    \item state-of-the-art integration/use development tools (CI/CD).
\end{itemize}

\section{Cern Tape Arcive}
\label{sec:cta}

The CERN Tape Archive (CTA)\cite{cta} is an open-source storage system developed by the CERN IT storage group to
replace the legacy CASTOR system that is used to manage the experiments' data on tapes. Its architecture is designed
to meet the requirements of LHC Run 3 as well as HL-LHC, thus needing the most data-intensive scientific workloads.

The CTA has two key components: frontend and tape daemon (Fig. \ref{fig:cta_overview}). The frontend accepts requests, like archive, retrieve, delete or cancel, from the attached disk storage and puts them into a request queue. If needed, the file catalogue is updated. The scheduling logic is embedded into the tape daemons, which are per-tape drive processes and seek matching tasks in the request queue. Once the desired number of requests is collected, the data are moved between disk and tape media using an embedded xroot\cite{xrootd} client.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.25]{cta-design.png}
    \caption{CTA design overview}
    \label{fig:cta_overview}
\end{figure}

Out of the box, CTA comes with a frontend that communicates with EOS, the disk system deployed at CERN. However, as CTA's queuing system is not EOS aware, other frontend implementations are possible. Moreover, the CTA project is actively working on flexibility in the system to allow wider adoption by other sites, ease of contributions from other developers and drop of CERN-specific dependencies in the provided binary packages \cite{cta_beyond_cern}.

For trigger data migration between disk and tape, CTA supports two request types: \textit{Archive} and \textit{Retrieve}. The retrieve operation can be cancelled with \textit{Cancel Retrieve} request. The \textit{Delete} request is used to cancel the archive operation or/and to remove an existing file from the tape catalogue. In the case of the EOS disk frontend all those operations are tied to EOS filesystem events, like \textit{CREATE}, \textit{CLOSE}, \textit{READ} and \textit{DELETE}. The requests are encoded with Google's protobuf \cite{prtobuf} and sent from EOS server to CTA frontend over Xroot Scalable Service Interface (XrootdSSI). In addition to data migration requests the frontend supports operations required by admin-cli tool, like \textit{define tape pool} or \textit{enable/disable tape drive}.

The CTA scheduler queue is backed by the CEPH object store or by a shared filesystem. This allows hundreds of thousands of requests to be collected for optimal scheduling decisions.

\section{dCache Integration with CTA}
\label{sec:integration}

dCache has a flexible tape interface which allows connectivity to any tape system. There are two ways that a file can be migrated to tape. Either dCache calls an HSM-specific copy command or through interaction via an in-dCache HSM-specific driver, called a nearline storage provider. The latter has been shown (by NDGF, TRIUMF and KIT Tier-1s), to provide better resource utilization and efficiency\cite{endit_kit}. Thus, for seamless integration of CTA the dCache developers at DESY have implemented a CTA-specific nearline storage provider, called dcache-cta\cite{dcache_cta}, and a corresponding frontend component for CTA. The communication between dCache and the new frontend is based on Googleâ€™s gRPC library.

\begin{lstlisting}[label=grpc-frontend,caption={gRPC service definition},keywords={service,rpc,returns}]
    service CtaRpc
    {
      rpc Create (Request) returns (CreateResponse) {}
      rpc Archive (Request) returns (ArchiveResponse) {}
      rpc Retrieve (Request) returns (RetrieveResponse) {}
      rpc Delete (Request) returns (Empty) {}
      rpc CancelRetrieve (Request) returns (Empty) {}
    
      rpc Version (Empty) returns (Version) {}
    }
\end{lstlisting}

The gRPC-based service is limited to data migration operations, thus the XrootdSSI-based frontend is needed to provide admin-tool functionality\footnote{There is ongoing development to migrate the admin interface to gRPC-based\cite{grpc} frontend. This activity is not directly correlated to dCache with CTA integration}.

As CTA has its own scheduler, to avoid double scheduling,  the flush and restore request queuing at the dCache pools should be disabled. In this way, all HSM requests from dCache are directly submitted to the CTA scheduler.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.25]{dcache-cta-integration.png}
    \caption{dCache integration with CTA}
    \label{fig:dcache_integration}
\end{figure}

\section{Migration from OSM}
\label{migraion}

DESY has a long history of providing HSM interface for scientific data. Today we have about 170PB on the tape media. Obviously, the migration to CTA should avoid physical file copies from one tape to another and be performed only by conversion of the existing OSM database schema to the CTA catalogue format. The migration procedure was split into three steps:
\begin{itemize}
    \item database scheme migration,
    \item update CTA tape server to support OSM-formated tapes,
    \item update file HSM-related metadata in dCache with CTA-relevant reference.
\end{itemize}

To add OSM tape format support to CTA, the CTA catalogue was extended to understand 
which format is used by a particular tape. This information is then propagated to the tape servers, which use a format-specific reader\footnote{The writing in the old format is not supported and all migrated tapes are marked as read-only.}.

Despite the fact, that OSM and CTA have completely different high-level designs both system catalogues are pretty similar (Fig.\ref{fig:osm_entry_example} and Fig.\ref{fig:cta_entry_example}) and require very few adjustments during the scheme migration.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.60]{osm-schema.png}
    \caption{Example of file metadata in OSM catalogue.}
    \label{fig:osm_entry_example}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.60]{cta-schema.png}
    \caption{Example of file metadata in CTA catalogue.}
    \label{fig:cta_entry_example}
\end{figure}

The link between files stored in dCache and their location on a tape relies on a so-called \textit{HSM-URI} - a URL that specifies the HSM type and an HSM type-specific identifier, opaque for dCache, that is generated by HSM when data are written to tape:

~\\
tape location $\Rightarrow$ $\begin{cases}
    osm://<instance>/111.222.333 &\text{for } OSM \\
    cta://<instance>/archiveid=123 &\text{for } CTA
\end{cases}$
~\\

dCache automatically will use the correct HSM setup based on the HSM-type in the tape location URI.

Since dCache can support multiple tape systems in parallel, the migration from OSM to CTA is performed experiment by experiment. As of today (August 2023), almost all experiments have already migrated and newly created data is being written on tape with CTA.

\section{First operational experience}
\label{experence}

The current deployment at DESY provides a single CTA instance shared between all dCache instances. We use four LTO-9 or IBM TS1160 tape drives attached per single tape server node. The hosts are equipped with 100 Gb/s, which is more than sufficient for the system's theoretical maximum streaming rate of 12.8Gb/s (4 x 400 MiB/s). During operation, the observed average drive speed was around 350MiB/s.  An NFS-mounted volume is used as the backend for the scheduler queue and easily handles a large number of archive and restore requests.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.30 ]{cta-deployment-desy.png}
    \caption{dCache with CTA. Deployment at DESY}
    \label{fig:dcache_cta_deplyment}
\end{figure}

The biggest challenge for the operational team was(is) the differences with existing OSM installations. Despite the fact that CTA deployment is simpler to operate\footnote{This is of course a subjective impression of the team members.}, new unfamiliar error situations have been observed. During the first month of operation, many modifications to the dcache-cta and cta-grpc-frontend have been applied. For example: added option to control the number of gRPC server threads, that process incoming requests on CTA frontend; implemented dCache $\leftrightarrow$ CTA communication timeout; increased dCache internal HSM requests lifetimes; the number of open file descriptors have been updated to match the number of gRPC requests. We anticipate that this list will expand as our experience with CTA continues to mature.

\section{Summary and future work}
\label{summary}

Today, we are pretty confident that dCache+CTA will address the data archival requirements of HEP, Photon Science and EuXFEL at DESY. The system has proven to provide the expected data rates and functionality. The migration from OSM to CTA was started in the spring of 2023 and is expected to be completed by the end of 2023.

All changes that are required to run dCache with CTA are part of the standard dCache builds (starting from dCache version 7.2) and public CTA releases (starting from {4,5}.7.12). The setup at DESY is replicated by PIC in Barcelona and Fermilab, in the USA. Both sites currently run Enstore\cite{enstore} and plan to migrate to dCache+CTA in 2024.
 
The new gRPC-based interface has shown its simplicity and functionality and is under evaluation by the EOS-CTA team for adoption.

\bibliography{bibliography}
\end{document}
