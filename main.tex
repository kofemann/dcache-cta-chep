%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a template file for Web of Conferences Journal
%
% Copy it to a new file with a new name and use it as the basis
% for your article
%
%%%%%%%%%%%%%%%%%%%%%%%%%% EDP Science %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%\documentclass[option]{webofc}
%%% "twocolumn" for typesetting an article in two columns format (default one column)
%
\documentclass{webofc}
\usepackage[varg]{txfonts}   % Web of Conferences font

\usepackage{hyperref}

\usepackage{pgfgantt}
\usepackage{capt-of}
\usepackage{listings}

\lstset{
  frame=single,                    % frame around the code
  %numbers=left,                   % where to put the line-numbers
  stepnumber=1,                   % the step between two line-numbers.
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=false,                % sets automatic line breaking
  breakatwhitespace=true,         % sets if automatic breaks should only happen at whitespace
}


%
% Put here some packages required or/and some personnal commands
%
%
\begin{document}

%
\title{dCache integration with CERN Tape Archive}
%
% subtitle is optionnal
%
%%%\subtitle{Do you have a subtitle?\\ If so, write it here}

\author{
         \firstname{Tigran} \lastname{Mkrtchyan}\inst{1}\fnsep\thanks{\email{tigran.mkrtchyan@desy.de}} \and
        \firstname{Jacek} \lastname{Chodak}\inst{1}\fnsep \and
        \firstname{Mwai} \lastname{Karimi}\inst{1}\fnsep \and
        \firstname{Ralf} \lastname{Lueken}\inst{1}\fnsep \and
        \firstname{Svenja} \lastname{Meyer}\inst{1}\fnsep \and
        \firstname{Peter} \lastname{Suchowski}\inst{1}\fnsep \and
        \firstname{Christian} \lastname{Voss}\inst{1}\fnsep
}

\institute{
Deutsches Elektronen-Synchrotron DESY, Notkestrasse 85, 22607 Hamburg, Germany
}

\abstract{%
  The ever-increasing amount of data that is produced by modern scientific facilities like EuXFEL or LHC puts
  high pressure on the data management infrastructure at the laboratories. This includes poorly shareable resources
  of archival storage, typically, tape libraries. To achieve maximal efficiency of the available tape resources a
  deep integration between hardware and software components is required. The CERN Tape Archive (CTA) is an open-source storage management system developed by CERN to manage LHC experiment
  data on tape. Although today CTA's primary target is CERN Tier-0, the data management group at DESY considers the
  CTA is a main alternative to commercial HSM systems. dCache has an extensible tape interface that allows connectivity almost to any tape system. Together with the CERN Tape Archive developers, the storage team at DESY works on seamless integration of CTA into dCache.
}
%
\maketitle
%
\section{Introduction}
\label{intro}

The ever-increasing amount of data that is produced by modern scientific facilities like EuXFEL or LHC puts high pressure
on the data management infrastructure at the laboratories. The challenges that have to be addressed span over the full data 
life-cycle: from ingest, efficient data analysis, up to long-term preservation. The latter typically utilizes magnetic
tape media, which in combination with disk storage covers the data management requirements. DESY-IT groups use dCache\cite{dcache}, a
storage system developed at DESY in collaboration with Fermilab and Nordic eInfrastructure Collaboration (NeIC), to manage
large numbers of disk servers and transparent data migration to and from archival storage.

Even though today large hard disk-based storage systems are cost, space and volume-effective (1 PB disk storage requires less than a ${1m^{3}}$ of space),
magnetic tapes are still the best option for long-term data archival, especially for so-called cold data, data that is rarely accessed.
Indeed, tape cartridges don't require electric power when not used, can store up to 20TB of uncompressed data and are designed for
15 - 30 years of archival storage. To organize a large number of tape cartridges, tape libraries with robotic arms are used.

There are, of course, some downsides of tape-based technologies. Though, when streaming a modern tape drive provides up to 400 MB/s,
the so-called positioning time on average is about 50 seconds. This doesn't include the mount time, the time which is needed by the
robotic arm to put the tape into an appropriate tape drive. Moreover, magnetic tapes don't support concurrency, which means that only
one stream, e.g. only one file, can be read or written at any point in time.

Thus, to achieve maximal efficiency of the available tape resources, e.g. maximize the time during which tape is read or written and
maximal speed, a deep integration between hardware and software components is required.

Since the early 90's DESY-IT has been relying on Open Storage Manager (OSM) software to operate tape libraries and access data on tapes\cite{osm_desy}.
During these three decades, the OSM underwent many changes. Over 80\% of the original code base has been updated to address computer, network,
operating system and software paradigms that have changed over decades. Nevertheless, the scaling capabilities put into the original design
don't apply today. Moreover, the commercial software license of OSM doesn't allow us to share our changes with the broader scientific community.
Thus, a new scalable and preferably open-source success is required.

With dCache as the primary disk layer in front of an HSM, the tape system should fulfill the following requirements:

\begin{itemize}
    \item Maximize tape HW efficiency
    \item Integration into DESY ecosystem
    \item Integration with dCache tape interface
    \item Support Enterprise \& LTO
    \item Daily turnover about 1PB
    \item Stable operation for the next decade
    \item Should be Open-source, adopting open standards
    \item Wide user and technology community
    \item State-of-the-art integration/use development tools (CI/CD)
\end{itemize}

\section{Cern Tape Arcive}
\label{sec:cta}

The CERN Tape Archive (CTA)\cite{cta} is an open-source storage system developed by the CERN IT storage group to
replace the legacy CASTOR system that is used to manage experiments data on tape. Its architecture is designed
to meet the requirements of LHC Run 3 as well as HL-LHC, thus needing the most data-intensive scientific workloads.

The CTA has two key components: frontend and tape daemon\ref{fig:cta_overview}. The frontend accepts the requests, like archive, retrieve, delete or cancel, from the attached disk storage and puts them into the request queue. If needed, the file catalog is updated. The scheduling logic is embedded into the tape daemons, which are per-tape drive processes and seek matching tasks in the request queue. Once the desired number of requests is collected, the data is moved between disk and tape media using embedded xroot\cite{xrootd} client.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.25]{cta-design.png}
    \caption{CTA design overview}
    \label{fig:cta_overview}
\end{figure}

Out of the box, CTA comes with a frontend that communicates with EOS, the disk system deployed at CERN. However, as CTA's queuing system is not EOS aware, other frontend implementations are possible. Moreover, the CTA project is actively working on flexibility in the system to allow wider adoption by other sites, ease of contributions from other developers and drop of CERN-specific dependencies in the provided binary packages \cite{cta_beyond_cern}.

For trigger data migration between disk and tape CTA supports two request types: \textit{Archive} and \textit{Retrieve}. The retrieve operation can be canceled with \textit{Cancel Retrieve} request. The \textit{Delete} request is used to cancel the archive operation or/and to remove an existing file from the tape catalog. In the case of the EOS disk frontend all those operations are tight to EOS filesystem events, like \textit{CREATE}, \textit{CLOSE}, \textit{READ} and \textit{DELETE}. The requests are encoded with Google's protobuf \cite{prtobuf} and sent from EOS server to CTA frontend over Xroot Scalable Service Interface (XrootdSSI). In addition to data migration requests the forntend supports operations required by admin-cli tool, like \textit{define tape pool} or \textit{enable/disable tape drives}.

The CTA scheduler queue is backed by the CEPH object store or by a shared filesystem. This allows hundreds of thousands of requests to be collected for optimal scheduling decisions.

\section{dCache Integration with CTA}
\label{sec:integration}

dCache has a flexible tape interface which allows connectivity to any tape system. There are two ways that a file can be migrated to tape. Ether dCache calls an HSM-specific copy command or through interaction via an in-dCache HSM-specific driver, called a nearline storage provider. The latter has been shown (by NDGF, TRIUMF and KIT Tier-1s), to provide better resource utilization and efficiency\cite{endit_kit}. Thus, for seamless integration of CTA the dCache developers at DESY have implemented a CTA-specific nearline storage provider, called dcache-cta\cite{dcache_cta}, and a corresponding frontend component for CTA. The communication between dCache and the new forntend is based on Googleâ€™s gRPC library.

\begin{lstlisting}[label=grpc-frontend,caption={gRPC service definition},keywords={service,rpc,returns}]
    service CtaRpc
    {
      rpc Create (Request) returns (CreateResponse) {}
      rpc Archive (Request) returns (ArchiveResponse) {}
      rpc Retrieve (Request) returns (RetrieveResponse) {}
      rpc Delete (Request) returns (Empty) {}
      rpc CancelRetrieve (Request) returns (Empty) {}
    
      rpc Version (Empty) returns (Version) {}
    }
\end{lstlisting}

The gRPC-based service is limited to data migration operations, thus the XrootdSSI-based frontend is needed to provide admin-tool functionality\footnote{There is an ongoing development to migrate the admin interface to gRPC-based\cite{grpc} frontend. This activity is not directly correlated to dCache with CTA integration}.

As CTA has its own scheduler, to avoid double scheduling,  the flush and restore request queuing at the dCache pools should be disabled. By this, all HSM requests from dCache are directly submitted to the CTA scheduler.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.25]{dcache-cta-integration.png}
    \caption{dCache integration with CTA}
    \label{fig:dcache_integration}
\end{figure}

\section{Migration from OSM}
\label{migraion}

DESY has a long history of providing HSM interface for scientific data. Today we have about XX PB on the tape media. Obviously, the migration to CTA should avoid physical file copies from one tape media to another and be performed only by conversion of the existing OSM database schema to the CTA catalog format. Additionally, the CTA tape servers must be able to read existing OSM-formed tapes and, finally, the files HSM reference in the dCache must be updated with CTA-relevant information.

To add OSM tape format support to CTA, the CTA catalog was extended to specify 
which format is used by a particular tape. This information is then propagated to the tape servers, which use a format-specific reader\footnote{The writing in the old format is not supported and all migrated tapes are marked as read-only.}.

Despite the fact, that OSM and CTA have completely different high-level designs both system catalogs are pretty similar (Fig.\ref{fig:osm_entry_example} and Fig.\ref{fig:cta_entry_example}) and require very few adjustments during the schema migration.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.60]{osm-schema.png}
    \caption{Example of file metadata in OSM catalog.}
    \label{fig:osm_entry_example}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.60]{cta-schema.png}
    \caption{Example of file metadata in CTA catalog.}
    \label{fig:cta_entry_example}
\end{figure}

Since dCache can support multiple tape systems in parallel, the migration from OSM to CTA is performed experiment by experiment. As of today (August 2023), almost all experiments have already migrated and newly created data is being written on tape with CTA.

\section{First operational experience}
\label{experence}


\begin{figure}[h]
    \centering
    \includegraphics[scale=0.30 ]{cta-deployment-desy.png}
    \caption{dCache with CTA. Deployment at DESY}
    \label{fig:dcache_cta_deplyment}
\end{figure}

\section{Summary and future work}
\label{summary}

Today, we are pretty confident that dCache+CTA will address the data archival requirements HEP, Photon Science and EuXFEL at DESY. We hope that other sites that have dCache and tape libraries or plan to have one will benefit from our work and experience. Thus, all activities are documented and shared with the dCache community.

The dCache+CTA is installed at DESY and under intensive testing. A test instance is connected to physical hardware and a production environment like load is applied. After three decades with OSM we have to gain new operational experience with CTA, understand system behaviour under high load, and recognize error cases. Moreover, the existing tape cartridges,which are written with OSM, should be recognized by CTA. A joint team of developers from Fermilab and DESY works together to extend CTA's tape server to support multiple tape formats.

\bibliography{bibliography.bib}
\end{document}
